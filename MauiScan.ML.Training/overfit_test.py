"""
è¿‡æ‹Ÿåˆæµ‹è¯• - åˆ¤æ–­æ˜¯"å­¦ä¸ä¼š"è¿˜æ˜¯"æ²¡å­¦å¯¹"
åœ¨å°‘é‡å›¾ç‰‡ï¼ˆ3-5å¼ ï¼‰ä¸Šç–¯ç‹‚è®­ç»ƒï¼Œçœ‹èƒ½å¦å®Œå…¨æ‹Ÿåˆ
"""

import sys
import os
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
from tqdm import tqdm
import json

# å¯¼å…¥æ¨¡å‹
sys.path.append(str(Path(__file__).parent / 'models'))
sys.path.append(str(Path(__file__).parent / 'dataset'))
from corner_detector import PPTCornerDetector, CornerDetectionLoss


class SmallDataset(Dataset):
    """å°æ•°æ®é›† - ç›´æ¥ä»æŒ‡å®šç›®å½•åŠ è½½"""

    def __init__(self, data_dir, input_size=512):
        self.data_dir = Path(data_dir)
        self.input_size = input_size
        self.samples = []

        # æŸ¥æ‰¾æ‰€æœ‰æœ‰æ ‡æ³¨çš„å›¾ç‰‡
        for img_path in self.data_dir.glob("*.png"):
            json_path = img_path.with_suffix('.json')
            if json_path.exists():
                self.samples.append((img_path, json_path))

        for img_path in self.data_dir.glob("*.jpg"):
            json_path = img_path.with_suffix('.json')
            if json_path.exists():
                self.samples.append((img_path, json_path))

        print(f"æ‰¾åˆ° {len(self.samples)} ä¸ªæ ·æœ¬")

        self.to_tensor = transforms.ToTensor()

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, json_path = self.samples[idx]

        # åŠ è½½å›¾ç‰‡
        image = Image.open(img_path).convert('RGB')
        original_width, original_height = image.size

        # åŠ è½½æ ‡æ³¨
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        corners = data['Corners']
        corners_array = np.array([[c['X'], c['Y']] for c in corners], dtype=np.float32)

        # å½’ä¸€åŒ–åæ ‡
        corners_norm = corners_array.copy()
        corners_norm[:, 0] /= original_width
        corners_norm[:, 1] /= original_height

        # ç¼©æ”¾å›¾ç‰‡
        image = image.resize((self.input_size, self.input_size), Image.BILINEAR)

        # è½¬æ¢ä¸º Tensor
        image_tensor = self.to_tensor(image)
        corners_tensor = torch.from_numpy(corners_norm).flatten().float()

        return image_tensor, corners_tensor, str(img_path.name)


def visualize_prediction(image_tensor, pred_coords, target_coords, output_path, epoch, pixel_error):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
    # è½¬æ¢ä¸º PIL Image
    image = transforms.ToPILImage()(image_tensor)
    draw = ImageDraw.Draw(image)

    size = 512

    # åå½’ä¸€åŒ–åˆ°åƒç´ åæ ‡
    pred_points = []
    target_points = []
    for i in range(0, 8, 2):
        pred_points.append((pred_coords[i] * size, pred_coords[i + 1] * size))
        target_points.append((target_coords[i] * size, target_coords[i + 1] * size))

    # ç”»çœŸå®æ ‡æ³¨ï¼ˆç»¿è‰²ï¼‰
    for i in range(4):
        draw.line([target_points[i], target_points[(i + 1) % 4]], fill='lime', width=4)

    # ç”»é¢„æµ‹ï¼ˆçº¢è‰²ï¼‰
    for i in range(4):
        draw.line([pred_points[i], pred_points[(i + 1) % 4]], fill='red', width=4)

    # ç”»è§’ç‚¹
    for pt in target_points:
        x, y = pt
        draw.ellipse([x - 10, y - 10, x + 10, y + 10], fill='lime', outline='white', width=2)

    for pt in pred_points:
        x, y = pt
        draw.ellipse([x - 10, y - 10, x + 10, y + 10], fill='red', outline='white', width=2)

    # æ·»åŠ æ–‡å­—è¯´æ˜
    draw.text((10, 10), f"Epoch {epoch}", fill='white')
    draw.text((10, 30), f"Pixel Error: {pixel_error:.2f}px", fill='yellow')
    draw.text((10, 50), "ç»¿è‰²=çœŸå®  çº¢è‰²=é¢„æµ‹", fill='white')

    image.save(output_path, quality=95)


def train_and_visualize(model, dataloader, criterion, optimizer, device, epoch, output_dir):
    """è®­ç»ƒä¸€ä¸ªepochå¹¶å¯è§†åŒ–æ¯ä¸ªæ ·æœ¬"""
    model.train()
    total_loss = 0.0
    pixel_errors = []

    for images, targets, filenames in dataloader:
        images = images.to(device)
        targets = targets.to(device)

        # å‰å‘ä¼ æ’­
        pred_coords, pred_conf = model(images)

        # è®¡ç®—æŸå¤±
        losses = criterion(pred_coords, pred_conf, targets)
        loss = losses['total_loss']

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()

        total_loss += loss.item()

        # è®¡ç®—åƒç´ è¯¯å·®
        pred_coords_np = pred_coords.detach().cpu().numpy()
        targets_np = targets.cpu().numpy()

        pred_pixels = pred_coords_np * 512
        target_pixels = targets_np * 512

        for i in range(len(images)):
            pred_pts = pred_pixels[i].reshape(4, 2)
            target_pts = target_pixels[i].reshape(4, 2)
            errors = np.linalg.norm(pred_pts - target_pts, axis=1)
            avg_error = np.mean(errors)
            pixel_errors.append(avg_error)

            # å¯è§†åŒ–ï¼ˆæ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ªepochï¼‰
            if epoch % 10 == 0 or epoch == 1:
                output_path = output_dir / f"epoch_{epoch:04d}_{filenames[i]}"
                visualize_prediction(
                    images[i].cpu(),
                    pred_coords_np[i],
                    targets_np[i],
                    str(output_path),
                    epoch,
                    avg_error
                )

    avg_loss = total_loss / len(dataloader)
    avg_pixel_error = np.mean(pixel_errors)

    return avg_loss, avg_pixel_error


def main():
    import argparse

    parser = argparse.ArgumentParser(description='è¿‡æ‹Ÿåˆæµ‹è¯•')
    parser.add_argument('--data-dir', type=str, default='../AnnotationTool/data/test',
                        help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--epochs', type=int, default=1000,
                        help='è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤1000ï¼‰')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¾å¤‡')
    parser.add_argument('--output-dir', type=str, default='overfit_results',
                        help='å¯è§†åŒ–ç»“æœè¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)

    print("="*60)
    print("  è¿‡æ‹Ÿåˆæµ‹è¯• - Overfit Test")
    print("="*60)
    print(f"æ•°æ®ç›®å½•: {args.data_dir}")
    print(f"è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"å­¦ä¹ ç‡: {args.lr}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("="*60)

    # åŠ è½½æ•°æ®
    print("\n[1/4] åŠ è½½æ•°æ®...")
    dataset = SmallDataset(args.data_dir, input_size=512)

    if len(dataset) == 0:
        print("[ERROR] æ²¡æœ‰æ‰¾åˆ°æ•°æ®ï¼")
        return

    print(f"æ ·æœ¬æ•°é‡: {len(dataset)}")

    # åˆ›å»º DataLoaderï¼ˆä¸æ‰“ä¹±ï¼Œæ¯æ¬¡éƒ½æ˜¯åŒæ ·çš„é¡ºåºï¼‰
    dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    print("\n[2/4] åˆ›å»ºæ¨¡å‹...")
    device = torch.device(args.device)
    model = PPTCornerDetector(pretrained=True).to(device)

    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    print("\n[3/4] è®¾ç½®è®­ç»ƒ...")
    criterion = CornerDetectionLoss(coord_weight=1.0, order_weight=0.5)
    optimizer = optim.Adam(model.parameters(), lr=args.lr)

    # å¼€å§‹è®­ç»ƒ
    print(f"\n[4/4] å¼€å§‹è¿‡æ‹Ÿåˆæµ‹è¯•ï¼ˆ{args.epochs} epochsï¼‰...")
    print("-"*60)

    best_error = float('inf')
    patience = 0
    max_patience = 100  # å¦‚æœ100è½®æ²¡æ”¹è¿›å°±åœæ­¢

    for epoch in range(1, args.epochs + 1):
        loss, pixel_error = train_and_visualize(
            model, dataloader, criterion, optimizer, device, epoch, output_dir
        )

        # æ‰“å°è¿›åº¦
        if epoch % 10 == 0 or epoch == 1:
            print(f"Epoch {epoch:4d}/{args.epochs} | Loss: {loss:.6f} | Pixel Error: {pixel_error:.3f}px")

        # æ£€æŸ¥æ˜¯å¦æ”¹è¿›
        if pixel_error < best_error:
            best_error = pixel_error
            patience = 0

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'pixel_error': pixel_error,
            }, output_dir / 'best_overfit_model.pth')
        else:
            patience += 1

        # æ—©åœï¼ˆå¦‚æœè¯¯å·®å·²ç»å¾ˆå°ï¼‰
        if pixel_error < 2.0:
            print(f"\nâœ… æˆåŠŸï¼åœ¨ç¬¬ {epoch} è½®è¾¾åˆ° {pixel_error:.2f}px è¯¯å·®")
            print("ğŸ‘‰ ç»“è®º: æ¨¡å‹æ¶æ„å’ŒæŸå¤±å‡½æ•°æ²¡é—®é¢˜ï¼Œé—®é¢˜åœ¨äºæ•°æ®é‡/åˆ†å¸ƒ")
            break

        # æ—©åœï¼ˆå¦‚æœé•¿æ—¶é—´ä¸æ”¹è¿›ï¼‰
        if patience >= max_patience:
            print(f"\nâš ï¸  è­¦å‘Šï¼š{max_patience} è½®æ²¡æœ‰æ”¹è¿›ï¼Œåœæ­¢è®­ç»ƒ")
            print(f"æœ€ä½³è¯¯å·®: {best_error:.2f}px")
            if best_error > 20:
                print("ğŸ‘‰ ç»“è®º: è¿å°‘é‡æ ·æœ¬éƒ½å­¦ä¸ä¼šï¼Œé«˜åº¦æ€€ç–‘æœ‰bug:")
                print("   - åæ ‡å½’ä¸€åŒ–/åå½’ä¸€åŒ–é—®é¢˜")
                print("   - è§’ç‚¹é¡ºåºä¸ä¸€è‡´")
                print("   - æŸå¤±å‡½æ•°æƒé‡é—®é¢˜")
                print("   - æ¨¡å‹è¾“å‡ºç»´åº¦é”™è¯¯")
            break

    print("\n"+"="*60)
    print("è¿‡æ‹Ÿåˆæµ‹è¯•å®Œæˆ!")
    print(f"æœ€ä½³åƒç´ è¯¯å·®: {best_error:.2f}px")
    print(f"å¯è§†åŒ–ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*60)

    # æœ€ç»ˆå¯è§†åŒ–
    print("\nç”Ÿæˆæœ€ç»ˆå¯è§†åŒ–...")
    model.eval()
    with torch.no_grad():
        for images, targets, filenames in dataloader:
            images = images.to(device)
            pred_coords, _ = model(images)

            pred_coords_np = pred_coords.cpu().numpy()
            targets_np = targets.cpu().numpy()

            pred_pixels = pred_coords_np * 512
            target_pixels = targets_np * 512

            for i in range(len(images)):
                pred_pts = pred_pixels[i].reshape(4, 2)
                target_pts = target_pixels[i].reshape(4, 2)
                errors = np.linalg.norm(pred_pts - target_pts, axis=1)
                avg_error = np.mean(errors)

                output_path = output_dir / f"FINAL_{filenames[i]}"
                visualize_prediction(
                    images[i].cpu(),
                    pred_coords_np[i],
                    targets_np[i],
                    str(output_path),
                    epoch,
                    avg_error
                )

                print(f"  {filenames[i]}: {avg_error:.2f}px")


if __name__ == '__main__':
    main()
